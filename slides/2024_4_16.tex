\documentclass{beamer}

\usetheme{CambridgeUS}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{CJKutf8}
\usepackage{datetime}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{ulem}

\usetikzlibrary{positioning, quotes}

\newdate{date}{16}{04}{2024}
\date{\displaydate{date}}
\title[Fairness]{Fairness in Machine Learning}
\author[Rex]{\begin{CJK}{UTF8}{bsmi}袁至誠\end{CJK}\\Chih-cheng Rex Yuan}
\institute[IIS]{Institute of Information Science}

\usetikzlibrary{arrows,shapes,fit}
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\DeclarePairedDelimiter{\set}{\{}{\}}
\DeclarePairedDelimiter{\tuple}{(}{)}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\let\oldleq\leq
\renewcommand{\leq}[1][]{\oldleq_{#1}}
\renewcommand{\implies}{\rightarrow}

\newcommand{\bye}[1]{}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\sred}[1]{\textcolor{red}{\sout{#1}}}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
    \frametitle{Bias}
    \begin{itemize}
        \item Decision making by machine learning may be biased.
        \item Bias can come from several sources:
        \begin{itemize}
            \item Biased data. ML is deisgned to replicate this.
            \item Missing data. The datasets might not be representative.
            \item Biased algorithms. The objective functions might introduce bias.
            \item Sensitive attributes: Age, Gender, ..., etc..
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Protected Attributes}
    What are the protected(sensitive) attributes?
    \begin{table}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \red{Age} & \red{Gender} & Occupation & \red{Income} & Education \\
            \hline
            \red{28} & \red{M} & Engineering & \red{\$80,000} & Master \\
            \red{28} & \red{F} & Engineering & \red{\$65,000} & Master \\
            \red{45} & \red{M} & Medicine    & \red{\$100,000} & Doctorate \\
            \red{40} & \red{F} & Legal       & \red{\$150,000} & Law Degree \\
            \red{32} & \red{M} & Education   & \red{\$55,000} & Bachelor \\
            \hline
        \end{tabular}
        \caption{Example Dataset}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{Fairness Through Unawareness}
    The most straightforward solution to fairness seems to be that
    just simply dropping all the protected columns.
    \begin{itemize}
        \item This is called \textit{Fairness Through Unawareness}.
        \item Formally it's
        \[
            X_i = X_j \implies \hat{Y}_i = \hat{Y}_j
        \]
        where $i,j$ are individuals; $X$ is the set of attributes
        except protected attributes; and $\hat{Y}$ is the prediction.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Fairness Through Unawareness}
    The downside of this is there could still be ``proxy'' attributes
    that correlate with protected attributes: like Occupation still
    correlates with Income.
    \begin{table}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \sout{Age} & \sout{Gender}   & \red{Occupation}  & \red{\sout{Income}}   & Education \\
            \hline
            \sout{28} & \sout{M} & \red{Engineering} & \red{\sout{\$80,000}} & Master \\
            \sout{28} & \sout{F} & \red{Engineering} & \red{\sout{\$65,000}} & Master \\
            \sout{45} & \sout{M} & \red{Medicine}    & \red{\sout{\$100,000}} & Doctorate \\
            \sout{40} & \sout{F} & \red{Legal}       & \red{\sout{\$150,000}} & Law Degree \\
            \sout{32} & \sout{M} & \red{Education}   & \red{\sout{\$55,000}} & Bachelor \\
            \hline
        \end{tabular}
        \caption{Example Dataset}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{Disparate Impact}
    \begin{itemize}
        \item In 1971, the US spreme court ruled it is illegal for hiring decisions to
        have ``disparate impact'' by race. It is taken as unintentional
        discrimination (as opposed to ``disparate treatment'', intentional discrimination).
        \item Legal issues involving disparate impact usually refer to the ``80\% Rule'',
        advocated by the US Equal Employment Opportunity Commission, where the
        selection rate of a minority group is to be no less than 80\% of that of
        a majority group.
        \item Formally, it requires that
        \[
            \frac{P[\hat{Y} = 1 | S \neq 1]}{P[\hat{Y} = 1 | S = 1]} \geq 1 - \epsilon
        \]
        where $\hat{Y} = 1$ represents acceptance(positive);
        $S = 1$ represents privileged group;
        $S \neq 1$ represents unprivileged group where $S$ is some protected
        attributes.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Disparate Impact}
    For example, if for some job opening there are $10$ female applicants and
    $100$ male applicants, and there are $2$ accepted females
    and $90$ accepted males. The measure does not hold for $\epsilon = 0.2$ because
    \[
        \frac{2/10}{80/100} = 0.25 \not\geq 0.8
    \]
    while if there were $9$ accepted females then it does hold because
    \[
        \frac{9/10}{80/100} = 1.125 \geq 0.8
    \]
\end{frame}

\begin{frame}
    \frametitle{Demographic Parity}
    \begin{itemize}
        \item Demographic parity is similar to disparate impact but, instead of
        ratio, difference is taken.
        \item Formally, it requires that
        \[
            \abs{P[\hat{Y} = 1 | S = 1] - P[\hat{Y} = 1 | S \neq 1]} \leq \epsilon
        \]
        where $\hat{Y} = 1$ represents acceptance(positive);
        $S = 1$ represents privileged group;
        $S \neq 1$ represents unprivileged group where $S$ is some protected
        attributes.
        \item This is usually known as affirmative action.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Disparate Impact and Demographic Parity}
    If for some job opening there are $10$ female applicants and
    $100$ male applicants, and there are $8$ accepted females
    and $90$ accepted males:
    \[
        \frac{8/10}{90/100} = 0.\overline{8} \quad\quad\quad
        \abs{8/10 - 90/100} = 0.1
    \]
    while if there were $1$ accepted females and $20$ accepted males:
    \[
        \frac{1/10}{20/100} = 0.5 \quad\quad\quad
        \abs{1/10 - 20/100} = 0.1
    \]
\end{frame}

\begin{frame}
    \frametitle{Disparate Impact and Demographic Parity}
    Disadvantage of these two measures:
    \begin{itemize}
        %% TODO: clarity
        \item ``A fully accurate classifier
        may be considered unfair, when the base rates (i.e., the proportion of
        actual positive outcomes) of the various groups are significantly different.''
        \item ``The notion permits that we accept the qualified applicants in
        one demographic, but random individuals in another, so long as the
        percentages of acceptance match.''
        \item It ``often cripples the utility that we might hope to achieve,
        especially in the common scenario in which an outcome to be predicated,
        e.g. whether the loan be will defaulted, is correlated
        with the protected attribute''
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Equalized Odds}
    \begin{itemize}
        \item Equalized odds is designed to address the downsides of the previous two
        by taking into accounts the actual ``ground truths'' and consider
        the difference between false-positive rates and true-positive rates of
        the groups.
        \item Formally, it requires that
        \begin{align*}
            \abs{P[\hat{Y} = 1 | S = 1, Y = 0] - P[\hat{Y} = 1 | S \neq 1, Y = 0]} & \leq \epsilon \\
            \abs{P[\hat{Y} = 1 | S = 1, Y = 1] - P[\hat{Y} = 1 | S \neq 1, Y = 1]} & \leq \epsilon
        \end{align*}
        where $Y$ represents ground truths.
        %% TODO: clarity
        \item A fully accurate classifier
        will necessarily satisfy the two equalized odds constraints.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Equal Opportunity}
    \begin{itemize}
        \item Equal opportunity is similar to equalized odds but focuses
        on true-positive rates of the groups only.
        \item Formally, it requires that
        \begin{align*}
            \abs{P[\hat{Y} = 1 | S = 1, Y = 1] - P[\hat{Y} = 1 | S \neq 1, Y = 1]} & \leq \epsilon
        \end{align*}
        where $Y$ represents ground truths.
        %% TODO: 倒果為因？
        \item It is a relaxation of equalized odds that focuses on the
        typically considered ``advantaged'' group $Y = 1$. For example,
        it ``requires that people who pay back their loan, have an equal
        opportunity of getting the loan in the first place (without specifying
        any requirement for those that will ultimately default).''
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Individual Fairness}
    \begin{itemize}
        \item All of the above fairness measures consider fairness across groups,
        while individual fairness requires that
        similar individuals will be treated similarly.
        \item Formally, it requires that
        \begin{align*}
            \abs{P[\hat{Y}^{(i)} = y | X^{(i)}, S^{(i)}] - P[\hat{Y}^{(j)} = y | X^{(j)}, S^{(j)}]} & \leq \epsilon \text{ if } d(i,j) \approx 0
        \end{align*}
        where $i,j$ denotes two individuals; $S^{(\cdot)}$ refers to sensitive attributes;
        $X^{(\cdot)}$ refers to associated features; and $d(i, j)$ is a distance metric between individuals.
        %% TODO: what is X?
        \item ``This measure considers other individual attributes for defining fairness rather
        than just the sensitive attributes.''
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Other Measures}
    \begin{itemize}
        \item Overall accuracy equality:
        \[
            \abs{P[Y = \hat{Y} | S = 1] - P[Y = \hat{Y} | S \neq 1]} \leq \epsilon
        \]
        where $Y = \hat{Y}$ means that the prediction was correct.
        \item Predictive parity:
        \[
            \abs{P[Y = 1 | S = 1, \hat{Y} = 1] - P[Y = 1 | S \neq 1, \hat{Y} = 1]} \leq \epsilon
        \]
        This requires that the ``positive predictive values'' are similar across
        groups, meaning the probability of an individual with a positive prediction
        actually experiencing a positive outcome.
        \item Equal calibration:
        \[
            \abs{P[Y = 1 | S = 1, V = v] - P[Y = 1 | S \neq 1, V = v]} \leq \epsilon
        \]
        where $V$ is the predicted probability value. When $V$ is binary, this is equivalent to predictive parity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Other Measures}
    \begin{itemize}
        \item Conditional statistical parity:
        \[
            \abs{P[\hat{Y} = 1 | S = 1, L = l] - P[\hat{Y} = 1 | S \neq 1, L = l]} \leq \epsilon
        \]
        where $L$ is a set of additional ``legitimate'' factors; for example, black and white
        defendants who have the same number of prior convictions.
        \item Predictive equality:
        \[
            \abs{P[\hat{Y} = 1 | S = 1, Y = 0] - P[\hat{Y} = 1 | S \neq 1, Y = 0]} \leq \epsilon
        \]
        This is similar to equal opportunity, but instead of focusing only on
        true-positive, this focuses only on false-positive rates.
        \item Conditional use accuracy equality:
        \begin{align*}
            \abs{P[Y = 1 | S = 1, \hat{Y} = 1] - P[Y = 1 | S \neq 1, \hat{Y} = 1]} & \leq \epsilon \\
            \abs{P[Y = 0 | S = 1, \hat{Y} = 0] - P[Y = 0 | S \neq 1, \hat{Y} = 0]} & \leq \epsilon
        \end{align*}
        This is similar to predictive parity but it additionally requires
        negative predictive values to be similar across groups.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Other Measures}
    \begin{itemize}
        \item Treatment equality:
        \[
            \abs{\frac{FN_{S=1}}{FP_{S=1}} - \frac{FN_{S\neq 1}}{FP_{S\neq 1}}} \leq \epsilon
        \]
        This is similar to equalized odds but difference of ratio is taken instead.
        \item Balance for the positive class:
        \[
            \abs{E[V | Y = 1, S = 1] - E[V | Y = 1, S \neq 1]} \leq \epsilon
        \]
        where V is the predicted probability value.

    \end{itemize}
\end{frame}

%% TODO: COMPAS

\end{document}
